# TestForge Dashboard

> An interactive analytics platform for comparing Large Language Model performance in Java test case generation

---

## Overview

TestForge Dashboard is an open-source analytics platform designed to visualize and compare the performance of Large Language Models (LLMs) in generating Java test cases. Built as part of research on automated test generation, this dashboard provides interactive visualizations and detailed metrics across multiple dimensions including model type, prompt strategy, test type, and problem complexity.

The platform analyzes data from **4,401 Java test cases** generated by **4 different LLM models** (Llama 3.3 and Qwen variants), evaluated across various configurations to provide comprehensive insights into LLM test generation capabilities.

### Who Should Use This?

- **Researchers** studying LLM capabilities in software testing and code generation
- **Developers** interested in automated testing and LLM applications
- **Contributors** wanting to extend the platform or add new models/metrics
- **Educators** teaching software testing or AI in software engineering

---

## Features

### Interactive Filtering

- Filter by **LLM model** (Llama 3.3, Qwen variants)
- Filter by **prompt strategy** (zero-shot, few-shot, chain-of-thought)
- Filter by **test type** (standard, boundary, mixed)
- Filter by **complexity level** (easy, moderate, hard)
- Persistent filter state saved in localStorage

### Visualization Components

1. **LLM Comparison Chart** - Bar chart comparing model performance
2. **Complexity Trend Analysis** - Performance across difficulty levels
3. **Test Type Analysis** - Comparative analysis by test category
4. **Prompt Strategy Analysis** - Multi-dimensional prompt effectiveness
5. **Performance Heatmap** - 2D visualization of performance patterns
6. **Degradation Cards** - Metric degradation analysis
7. **Coverage-FC Gap Analysis** - Correlation between coverage and correctness
8. **Summary Cards** - Key performance metrics at a glance
9. **AI-Generated Insights** - Automated analysis and observations

### Data Tables

- **Test Set Metrics Table** - Suite-level success rates with sorting/filtering
- **Test Case Metrics Table** - Individual test quality metrics
- Built with TanStack Table for high-performance data operations

### Advanced Features

- Client-side filtering with full dimensional flexibility
- Responsive design for desktop and mobile
- Real-time filter updates
- Pre-aggregated data for accurate statistical representation
- Metric explanations and tooltips

---

## Metrics Explained

### Test Set Metrics (Suite-Level)

| Metric  | Full Name              | Description                                                                          |
| ------- | ---------------------- | ------------------------------------------------------------------------------------ |
| **CSR** | Compile Success Rate   | Percentage of generated tests that compiled successfully                             |
| **RSR** | Runtime Success Rate   | Percentage of compiled tests that executed successfully (ran on ≥80% of submissions) |
| **SVR** | Semantic Validity Rate | Percentage of runtime-successful tests marked as semantically valid                  |

### Test Case Metrics (Individual Test Quality)

| Metric       | Full Name              | Description                                                             |
| ------------ | ---------------------- | ----------------------------------------------------------------------- |
| **FC**       | Functional Correctness | Percentage of test cases passing on ≥80% of submissions                 |
| **Coverage** | Average Line Coverage  | Average code coverage achieved by the top 16 functionally correct tests |

---

## Tech Stack

### Core Framework

- **Next.js 15** - React framework with App Router and Turbopack
- **React 19.1** - UI library
- **TypeScript 5** - Type-safe development

### Data Visualization

- **Recharts 3.5** - Charting library for all visualizations
- **TanStack React Table 8.21** - High-performance data tables

### State & Data Management

- **Zustand 5.0** - Lightweight state management with localStorage persistence
- **Papa Parse 5.5** - CSV parsing and data loading
- **Zod 4.1** - Runtime type validation

### UI & Styling

- **Tailwind CSS 4.0** - Utility-first CSS framework
- **Radix UI** - Accessible component primitives (Select, Tabs, Slot)
- **Lucide React** - Icon library
- **Shadcn/ui** - Reusable component system

---

## Getting Started

### Prerequisites

- **Node.js** 20.x or higher
- **npm**, **yarn**, **pnpm**, or **bun** package manager

### Installation

1. Clone the repository:

```bash
git clone https://github.com/yourusername/testforge-dashboard.git
cd testforge-dashboard
```

2. Install dependencies:

```bash
npm install
# or
yarn install
# or
pnpm install
```

3. Run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
```

4. Open [http://localhost:3000](http://localhost:3000) in your browser

The dashboard will be running with hot module replacement enabled via Turbopack.

### Build for Production

```bash
npm run build
npm start
```

### Available Scripts

| Command         | Description                             |
| --------------- | --------------------------------------- |
| `npm run dev`   | Start development server with Turbopack |
| `npm run build` | Build production bundle with Turbopack  |
| `npm start`     | Start production server                 |
| `npm run lint`  | Run ESLint code linting                 |

---

## Project Structure

```
testforge-dashboard/
├── src/
│   ├── app/                              # Next.js App Router
│   │   ├── api/
│   │   │   ├── test-set-metrics/[aggregation]/route.ts
│   │   │   └── test-case-metrics/[aggregation]/route.ts
│   │   ├── layout.tsx
│   │   └── page.tsx                      # Main dashboard page
│   ├── components/
│   │   ├── analytics/                    # 9 chart/card components
│   │   ├── charts/                       # Shared chart components
│   │   ├── tables/                       # TanStack Table components
│   │   ├── filters/                      # Filter UI (FilterPanel, etc.)
│   │   ├── info/                         # Metric explanations
│   │   └── ui/                           # Shadcn/UI base components
│   ├── data/
│   │   ├── Test set metrics/             # tsm_full_config.csv
│   │   └── Test case metrics/            # tcm_full_config.csv
│   ├── lib/
│   │   ├── data/                         # CSV loading and aggregation logic
│   │   ├── insights/                     # AI insights generation
│   │   ├── validations/                  # Zod schemas
│   │   └── utils.ts                      # Utility functions
│   ├── hooks/
│   │   └── useFilters.ts                 # Zustand filter store
│   ├── types/
│   │   └── metrics.ts                    # TypeScript type definitions
│   └── config/
│       ├── constants.ts                  # LLM, prompt, test configs
│       └── design-tokens.ts              # Design system tokens
├── public/                               # Static assets
└── package.json
```

---

## Data Architecture

### Data Files

TestForge Dashboard loads data from **2 pre-aggregated CSV files**:

- **`tsm_full_config.csv`** - Test Set Metrics (108 rows: 4 LLMs × 3 prompts × 3 test types × 3 complexities)
- **`tcm_full_config.csv`** - Test Case Metrics (108 rows: 4 LLMs × 3 prompts × 3 test types × 3 complexities)

These files contain all possible combinations of the four dimensions (LLM model, prompt strategy, test type, and complexity), allowing the dashboard to display any chart or metric comparison across all dimensions.

### How It Works

1. **Dashboard loads** - Both `full_config` CSV files are loaded via API routes
2. **Filter state updated** - User selections stored in Zustand with localStorage persistence
3. **Client-side filtering** - Data is filtered based on active filters using `filterTestSetMetrics()` and `filterTestCaseMetrics()`
4. **Recharts & TanStack Table** - Render the filtered data in visualizations and tables

### Why Pre-aggregation?

Metrics like CSR, RSR, and SVR require weighted averaging across different dimensions. Pre-computing these aggregations ensures accurate statistical representation rather than naive arithmetic means. The full configuration dataset provides complete flexibility for all chart types while maintaining data integrity.

---

## Development

### Development Workflow

1. **Read before coding** - Understand existing patterns before making changes
2. **Match existing patterns** - Follow established conventions in the codebase
3. **Use existing utilities** - Check `lib/`, `config/`, `hooks/` before creating new helpers
4. **Test with actual data** - Inspect CSV files and API responses during development

### Code Patterns

- **Type Safety**: TypeScript interfaces for metrics in `types/metrics.ts`
- **CSV Normalization**: Handles inconsistent casing in source data via `lib/data/load-csv.ts`
- **Caching**: API routes use `force-static` with 1-hour revalidation
- **Client-side Filtering**: All filtering happens in the browser after data is loaded

### Important Files

- **Filter Logic**: `src/hooks/useFilters.ts` (Zustand store)
- **Data Loading**: `src/lib/data/load-csv.ts` (Papa Parse + normalization)
- **Filtering Logic**: `src/lib/data/filter-data.ts` (client-side filtering)
- **API Routes**: `src/app/api/test-set-metrics/[aggregation]/route.ts`
- **Constants**: `src/config/constants.ts` (LLM models, prompts, colors)

### Gotchas

- Next.js 15 requires `await params` in route handlers (async params)
- CSV files in `src/data/` are server-side only (loaded via API routes)
- Full dataset is loaded once, then filtered client-side for performance
- CSV files may have inconsistent casing ("Mixed" vs "mix", "Chain-of-Thought" vs "chain_of_thought")

---

## Configuration

### Adding New LLM Models

1. Add model configuration to `src/config/constants.ts`:

```typescript
export const LLM_MODELS = [
  // existing models...
  { id: "new-model", name: "New Model", color: "#hex-color" },
];
```

2. Update data files with new model data
3. Update type definitions in `src/types/metrics.ts` if needed

### Adding New Prompt Strategies

1. Add prompt strategy to `src/config/constants.ts`:

```typescript
export const PROMPT_STRATEGIES = [
  // existing strategies...
  { id: "new-strategy", name: "New Strategy", description: "..." },
];
```

2. Generate pre-aggregated CSV files including the new strategy
3. Update Zod schemas in `src/lib/validations/` if schema changes

### Modifying Metrics

1. Update type definitions in `src/types/metrics.ts`
2. Add/modify Zod schemas in `src/lib/validations/`
3. Update CSV normalization in `src/lib/data/load-csv.ts`
4. Update metric explanations in `src/components/info/`
5. Regenerate the `full_config` CSV files with the new metrics

---

## Contributing

We welcome contributions from the community! Whether you're fixing bugs, adding features, improving documentation, or proposing new metrics, your help is appreciated.

### How to Contribute

1. **Fork the repository** and create a new branch:

```bash
git checkout -b feature/your-feature-name
```

2. **Make your changes** following the code patterns in the codebase

3. **Test your changes** locally:

```bash
npm run dev
npm run lint
```

4. **Commit your changes** with clear, descriptive messages:

```bash
git commit -m "Add: description of your changes"
```

5. **Push to your fork** and submit a pull request:

```bash
git push origin feature/your-feature-name
```

### Development Setup for Contributors

- Follow existing code patterns (TypeScript strict mode, functional components)
- Keep changes minimal and focused
- Add comments only where logic isn't self-evident
- No over-engineering - match the existing abstraction level

### Code Quality Guidelines

- **TypeScript**: Strict mode enabled, no `any` types without justification
- **Formatting**: Follow existing code style (2-space indentation, single quotes for strings)
- **Components**: Functional components with TypeScript interfaces for props
- **Naming**: Clear, descriptive names (`determineAggregationLevel` not `getAgg`)
- **Comments**: Explain "why" not "what"

### Pull Request Process

1. Ensure your code passes linting (`npm run lint`)
2. Update documentation if you're changing functionality
3. Reference any related issues in your PR description
4. Be responsive to feedback and questions during code review

### Code of Conduct

This project follows standard open-source community guidelines. Be respectful, constructive, and welcoming to all contributors.

---

## Acknowledgments

### Research Context

This dashboard was developed to support research on Large Language Model capabilities in automated test generation. The dataset includes 4,401 Java test cases generated across multiple LLM models and evaluated using comprehensive quality metrics.

### Data Sources

- Test case generation data from LLM evaluation experiments
- Metrics computed from execution against real Java submissions
- Pre-aggregated statistics across multiple dimensions

### Contributors

Thank you to all contributors who have helped improve this project!

<!-- Add contributor badges/list here -->

---

## Support & Issues

### Reporting Bugs

If you encounter a bug, please open an issue on GitHub with:

- Clear description of the problem
- Steps to reproduce
- Expected vs actual behavior
- Screenshots if applicable
- Browser and OS information

### Feature Requests

Have an idea for a new feature? Open an issue with:

- Description of the feature
- Use case / why it's needed
- Proposed implementation (optional)

### Questions & Discussion

- **GitHub Issues**: For bug reports and feature requests
- **Discussions**: For questions and general discussion

---

## Resources

- **Next.js**: [https://nextjs.org/docs](https://nextjs.org/docs)
- **Recharts**: [https://recharts.org/](https://recharts.org/)
- **TanStack Table**: [https://tanstack.com/table/latest](https://tanstack.com/table/latest)

---

**Built with ❤️ for the software testing and LLM research community**
